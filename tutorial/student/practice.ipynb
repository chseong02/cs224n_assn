{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6]]\n",
      "[[1, 2, 3], [4, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "list_of_lists = [\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "]\n",
    "pp.pprint(list_of_lists)\n",
    "print(list_of_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(list_of_lists)\n",
    "pp.pprint(data)\n",
    "\n",
    "data = torch.tensor([[0, 1], [2, 3], [4, 5]])\n",
    "pp.pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1.],\n",
      "        [2., 3.],\n",
      "        [4., 5.]])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor([[0, 1], [2, 3], [4, 5]], dtype=torch.float32)\n",
    "pp.pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]], dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "zeros = torch.zeros(2, 5, dtype=torch.int16)\n",
    "pp.pprint(zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "rr = torch.arange(1, 10)\n",
    "pp.pprint(rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  4,  6,  8, 10, 12, 14, 16, 18])"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A is tensor([[1, 2],\n",
      "        [2, 3],\n",
      "        [4, 5]])\n",
      "B is tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "The product is tensor([[11, 14, 17, 20],\n",
      "        [17, 22, 27, 32],\n",
      "        [29, 38, 47, 56]])\n",
      "The other product is tensor([[11, 14, 17, 20],\n",
      "        [17, 22, 27, 32],\n",
      "        [29, 38, 47, 56]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2], [2, 3], [4, 5]])  # (3, 2)\n",
    "b = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])  # (2, 4)\n",
    "\n",
    "print(\"A is\", a)\n",
    "print(\"B is\", b)\n",
    "print(\"The product is\", a.matmul(b))  # (3, 4)\n",
    "print(\"The other product is\", a @ b)  # +, -, *, @"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  2,  3,  4],\n",
      "         [-2,  5,  6,  9]],\n",
      "\n",
      "        [[ 5,  6,  7,  2],\n",
      "         [ 8,  9, 10,  4]],\n",
      "\n",
      "        [[-3,  2,  2,  1],\n",
      "         [ 4,  6,  5,  9]]])\n",
      "torch.Size([3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "matr_3d = torch.tensor(\n",
    "    [\n",
    "        [[1, 2, 3, 4], [-2, 5, 6, 9]],\n",
    "        [[5, 6, 7, 2], [8, 9, 10, 4]],\n",
    "        [[-3, 2, 2, 1], [4, 6, 5, 9]],\n",
    "    ]\n",
    ")\n",
    "pp.pprint(matr_3d)\n",
    "pp.pprint(matr_3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape is currently torch.Size([15])\n",
      "The contents are currently tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n",
      "\n",
      "After reshaping, the shape is currently torch.Size([5, 3])\n",
      "The contents are currently tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12],\n",
      "        [13, 14, 15]])\n"
     ]
    }
   ],
   "source": [
    "rr = torch.arange(1, 16)\n",
    "print(\"The shape is currently\", rr.shape)\n",
    "print(\"The contents are currently\", rr)\n",
    "print()\n",
    "rr = rr.view(5, 3)\n",
    "print(\"After reshaping, the shape is currently\", rr.shape)\n",
    "print(\"The contents are currently\", rr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is: tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11., 12., 13., 14.],\n",
      "        [15., 16., 17., 18., 19., 20., 21.],\n",
      "        [22., 23., 24., 25., 26., 27., 28.],\n",
      "        [29., 30., 31., 32., 33., 34., 35.]])\n",
      "Taking the sum over rows:\n",
      "tensor([ 28.,  77., 126., 175., 224.])\n",
      "Taking thep sum over columns:\n",
      "tensor([ 75.,  80.,  85.,  90.,  95., 100., 105.])\n",
      "Taking the stdev over rows:\n",
      "tensor([2.1602, 2.1602, 2.1602, 2.1602, 2.1602])\n",
      "Taking the stdev over columns:\n",
      "tensor([11.0680, 11.0680, 11.0680, 11.0680, 11.0680, 11.0680, 11.0680])\n"
     ]
    }
   ],
   "source": [
    "data = torch.arange(1, 36, dtype=torch.float32).reshape(5, 7)\n",
    "print(\"Data is:\", data)\n",
    "\n",
    "# We can perform operations like *sum* over each row...\n",
    "print(\"Taking the sum over rows:\")\n",
    "print(data.sum(dim=1))  # (5,)\n",
    "\n",
    "# or over each column.\n",
    "print(\"Taking thep sum over columns:\")\n",
    "print(data.sum(dim=0))  # (7,)\n",
    "\n",
    "# Other operations are available:\n",
    "print(\"Taking the stdev over rows:\")\n",
    "print(data.std(dim=1))\n",
    "\n",
    "# Other operations are available:\n",
    "print(\"Taking the stdev over columns:\")\n",
    "print(data.std(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.2667, 1.0333])\n",
      "torch.Size([2])\n",
      "tensor([ 2.5000, -2.5000,  7.9500])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor([[1, 2.2, 9.6], [4, -7.2, 6.3]])\n",
    "data_row_mean = data.mean(dim=1)\n",
    "data_col_mean = data.mean(dim=0)\n",
    "pp.pprint(data_row_mean)\n",
    "pp.pprint(data_row_mean.shape)\n",
    "pp.pprint(data_col_mean)\n",
    "pp.pprint(data_col_mean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "pp.pprint(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "y = x * x * 3\n",
    "y.backward()\n",
    "pp.pprint(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([24.])\n"
     ]
    }
   ],
   "source": [
    "z = x * x * 3\n",
    "z.backward()\n",
    "pp.pprint(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0392,  0.3787,  0.1040],\n",
       "         [-0.0392,  0.3787,  0.1040],\n",
       "         [-0.0392,  0.3787,  0.1040]],\n",
       "\n",
       "        [[-0.0392,  0.3787,  0.1040],\n",
       "         [-0.0392,  0.3787,  0.1040],\n",
       "         [-0.0392,  0.3787,  0.1040]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the inputs\n",
    "input = torch.ones(2, 3, 4)\n",
    "# N* H_in -> N*H_out\n",
    "\n",
    "\n",
    "# Make a linear layers transforming N,*,H_in dimensinal inputs to N,*,H_out\n",
    "# dimensional outputs\n",
    "linear = nn.Linear(4, 3)\n",
    "linear_output = linear(input)\n",
    "linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 3, 5])"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(2 * 3 * 4).reshape((2, 3, 4))\n",
    "B = torch.arange(2 * 5 * 4 * 5).reshape((5, 2, 4, 5))\n",
    "C = torch.arange(20 * 5 * 5 * 2 * 4 * 6).reshape((20, 5, 5, 2, 4, 6))\n",
    "D = torch.arange(20 * 5 * 5 * 2 * 6 * 10).reshape((20, 5, 5, 2, 6, 10))\n",
    "torch.matmul(A, B).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0800, -0.1673,  0.0630,  0.1961],\n",
       "         [ 0.4169, -0.3202,  0.2804, -0.1184],\n",
       "         [ 0.0061,  0.1198,  0.4542, -0.4934]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2111,  0.1199,  0.0173], requires_grad=True)]"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(linear.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0392,  0.3787,  0.1040],\n",
       "         [-0.0392,  0.3787,  0.1040],\n",
       "         [-0.0392,  0.3787,  0.1040]],\n",
       "\n",
       "        [[-0.0392,  0.3787,  0.1040],\n",
       "         [-0.0392,  0.3787,  0.1040],\n",
       "         [-0.0392,  0.3787,  0.1040]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0004,  0.3787,  0.1040],\n",
       "         [-0.0004,  0.3787,  0.1040],\n",
       "         [-0.0004,  0.3787,  0.1040]],\n",
       "\n",
       "        [[-0.0004,  0.3787,  0.1040],\n",
       "         [-0.0004,  0.3787,  0.1040],\n",
       "         [-0.0004,  0.3787,  0.1040]]], grad_fn=<LeakyReluBackward0>)"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu = nn.LeakyReLU()\n",
    "output = relu(linear_output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4892, 0.4614],\n",
       "         [0.4892, 0.4614],\n",
       "         [0.4892, 0.4614]],\n",
       "\n",
       "        [[0.4892, 0.4614],\n",
       "         [0.4892, 0.4614],\n",
       "         [0.4892, 0.4614]]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = nn.Sequential(nn.Linear(4, 2), nn.Sigmoid())\n",
    "\n",
    "input = torch.ones(2, 3, 4)\n",
    "output = block(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4599, 0.4472, 0.3817, 0.5377, 0.5676],\n",
       "        [0.4556, 0.4115, 0.3895, 0.5498, 0.6125]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.input_size),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output\n",
    "\n",
    "\n",
    "input = torch.randn(2, 5)\n",
    "model = MultilayerPerceptron(5, 3)\n",
    "\n",
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model.0.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0322,  0.3580,  0.2086, -0.2141,  0.3808],\n",
       "          [ 0.1602,  0.3442, -0.3442,  0.1515,  0.3330],\n",
       "          [-0.3538, -0.0952,  0.0685,  0.3144,  0.3318]], requires_grad=True)),\n",
       " ('model.0.bias',\n",
       "  Parameter containing:\n",
       "  tensor([ 0.3614,  0.3961, -0.3199], requires_grad=True)),\n",
       " ('model.2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[ 0.0139, -0.1531,  0.0183],\n",
       "          [-0.3909,  0.1833, -0.3987],\n",
       "          [ 0.1113, -0.1064, -0.4570],\n",
       "          [-0.0267,  0.3975,  0.1471],\n",
       "          [ 0.4852, -0.1944,  0.5328]], requires_grad=True)),\n",
       " ('model.2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0956, -0.1618, -0.4720, -0.0214,  0.1946], requires_grad=True))]"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0328,  2.6049, -1.6300,  2.1344,  0.2627],\n",
      "        [ 0.7165,  2.1657,  1.8893,  0.4129,  0.9372],\n",
      "        [ 1.1807,  0.6802, -1.8700,  0.8503,  0.2655],\n",
      "        [ 0.9264,  0.8395,  2.2977,  0.2667,  1.3267],\n",
      "        [ 1.1307,  0.8110,  1.5679,  1.0221,  0.0785],\n",
      "        [ 0.5014,  0.5947,  2.8493,  1.5814,  2.3587],\n",
      "        [ 0.7254, -0.5478, -0.6408,  0.1701,  0.1744],\n",
      "        [ 0.4876,  2.1962,  0.7475,  1.7778,  3.5014],\n",
      "        [-0.6083,  1.8779,  2.0250,  2.0709, -0.4616],\n",
      "        [ 1.0606,  3.5305,  2.7873,  0.3350,  3.0163]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.ones(10, 5)\n",
    "z = torch.randn_like(y)\n",
    "x = y + z\n",
    "pp.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24508769810199738"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultilayerPerceptron(5, 3)\n",
    "\n",
    "adam = optim.Adam(model.parameters(), lr=1e-1)\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "y_pred = model(x)\n",
    "loss_function(y_pred, y).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: traing loss: 0.24508769810199738\n",
      "Epoch 1: traing loss: 0.18516084551811218\n",
      "Epoch 2: traing loss: 0.1216031089425087\n",
      "Epoch 3: traing loss: 0.06360343098640442\n",
      "Epoch 4: traing loss: 0.03253542631864548\n",
      "Epoch 5: traing loss: 0.01589490845799446\n",
      "Epoch 6: traing loss: 0.007973880507051945\n",
      "Epoch 7: traing loss: 0.003646576078608632\n",
      "Epoch 8: traing loss: 0.0016105301911011338\n",
      "Epoch 9: traing loss: 0.0007155617931857705\n",
      "Epoch 10: traing loss: 0.0003285373386461288\n",
      "Epoch 11: traing loss: 0.0001579771633259952\n",
      "Epoch 12: traing loss: 7.97101019998081e-05\n",
      "Epoch 13: traing loss: 4.20837095589377e-05\n",
      "Epoch 14: traing loss: 2.31685335165821e-05\n",
      "Epoch 15: traing loss: 1.3263455912237987e-05\n",
      "Epoch 16: traing loss: 7.878484211687464e-06\n",
      "Epoch 17: traing loss: 4.847102900384925e-06\n",
      "Epoch 18: traing loss: 3.0836142741463846e-06\n",
      "Epoch 19: traing loss: 2.025299863817054e-06\n",
      "Epoch 20: traing loss: 1.3710840676139924e-06\n",
      "Epoch 21: traing loss: 9.552164783599437e-07\n",
      "Epoch 22: traing loss: 6.837202590759262e-07\n",
      "Epoch 23: traing loss: 5.020235107622284e-07\n",
      "Epoch 24: traing loss: 3.775053585286514e-07\n",
      "Epoch 25: traing loss: 2.90288539872563e-07\n",
      "Epoch 26: traing loss: 2.2790241871462058e-07\n",
      "Epoch 27: traing loss: 1.824364943558976e-07\n",
      "Epoch 28: traing loss: 1.4870312270431896e-07\n",
      "Epoch 29: traing loss: 1.2324049691869732e-07\n",
      "Epoch 30: traing loss: 1.0373786807349461e-07\n",
      "Epoch 31: traing loss: 8.857156785779807e-08\n",
      "Epoch 32: traing loss: 7.663155088266649e-08\n",
      "Epoch 33: traing loss: 6.712011924037142e-08\n",
      "Epoch 34: traing loss: 5.945632963744174e-08\n",
      "Epoch 35: traing loss: 5.320799090213768e-08\n",
      "Epoch 36: traing loss: 4.807017006669412e-08\n",
      "Epoch 37: traing loss: 4.381771745443075e-08\n",
      "Epoch 38: traing loss: 4.0260566436245426e-08\n",
      "Epoch 39: traing loss: 3.7270421415769306e-08\n",
      "Epoch 40: traing loss: 3.47262343325383e-08\n",
      "Epoch 41: traing loss: 3.2564756224928715e-08\n",
      "Epoch 42: traing loss: 3.071072285365517e-08\n",
      "Epoch 43: traing loss: 2.910775442899194e-08\n",
      "Epoch 44: traing loss: 2.7723935147605516e-08\n",
      "Epoch 45: traing loss: 2.6514451079151513e-08\n",
      "Epoch 46: traing loss: 2.5465624275966547e-08\n",
      "Epoch 47: traing loss: 2.4545750321181004e-08\n",
      "Epoch 48: traing loss: 2.373780283448923e-08\n",
      "Epoch 49: traing loss: 2.3024163908758055e-08\n",
      "Epoch 50: traing loss: 2.2391814624711515e-08\n",
      "Epoch 51: traing loss: 2.183334402161563e-08\n",
      "Epoch 52: traing loss: 2.1338655287195252e-08\n",
      "Epoch 53: traing loss: 2.0898241359645908e-08\n",
      "Epoch 54: traing loss: 2.050596847880115e-08\n",
      "Epoch 55: traing loss: 2.015294775503662e-08\n",
      "Epoch 56: traing loss: 1.9842151033344635e-08\n",
      "Epoch 57: traing loss: 1.9558132891006608e-08\n",
      "Epoch 58: traing loss: 1.9306096277205143e-08\n",
      "Epoch 59: traing loss: 1.9079175572755958e-08\n",
      "Epoch 60: traing loss: 1.8877157614838325e-08\n",
      "Epoch 61: traing loss: 1.8695278214408972e-08\n",
      "Epoch 62: traing loss: 1.853143949404057e-08\n",
      "Epoch 63: traing loss: 1.838406049614605e-08\n",
      "Epoch 64: traing loss: 1.8253873079743244e-08\n",
      "Epoch 65: traing loss: 1.813161532027152e-08\n",
      "Epoch 66: traing loss: 1.8024209680334025e-08\n",
      "Epoch 67: traing loss: 1.79270145395094e-08\n",
      "Epoch 68: traing loss: 1.7837251675700827e-08\n",
      "Epoch 69: traing loss: 1.7760307002845366e-08\n",
      "Epoch 70: traing loss: 1.7689313125401895e-08\n",
      "Epoch 71: traing loss: 1.7620715553334776e-08\n",
      "Epoch 72: traing loss: 1.756493439586393e-08\n",
      "Epoch 73: traing loss: 1.7510588534719318e-08\n",
      "Epoch 74: traing loss: 1.7462818746594166e-08\n",
      "Epoch 75: traing loss: 1.7417709941014436e-08\n",
      "Epoch 76: traing loss: 1.737999077988661e-08\n",
      "Epoch 77: traing loss: 1.7342783209528534e-08\n",
      "Epoch 78: traing loss: 1.731246790370733e-08\n",
      "Epoch 79: traing loss: 1.7285225695218287e-08\n",
      "Epoch 80: traing loss: 1.7254931705679155e-08\n",
      "Epoch 81: traing loss: 1.7231512217108502e-08\n",
      "Epoch 82: traing loss: 1.7210089353625335e-08\n",
      "Epoch 83: traing loss: 1.719070930050748e-08\n",
      "Epoch 84: traing loss: 1.7173064748021716e-08\n",
      "Epoch 85: traing loss: 1.7155910470023628e-08\n",
      "Epoch 86: traing loss: 1.714289687981818e-08\n",
      "Epoch 87: traing loss: 1.7128812146438577e-08\n",
      "Epoch 88: traing loss: 1.7115809214374167e-08\n",
      "Epoch 89: traing loss: 1.7105477922996215e-08\n",
      "Epoch 90: traing loss: 1.7094924587013338e-08\n",
      "Epoch 91: traing loss: 1.708413499557082e-08\n",
      "Epoch 92: traing loss: 1.707842045561847e-08\n",
      "Epoch 93: traing loss: 1.7071151603431645e-08\n",
      "Epoch 94: traing loss: 1.706177954474697e-08\n",
      "Epoch 95: traing loss: 1.7054510692560143e-08\n",
      "Epoch 96: traing loss: 1.705184971001472e-08\n",
      "Epoch 97: traing loss: 1.7046779987595073e-08\n",
      "Epoch 98: traing loss: 1.704107255307008e-08\n",
      "Epoch 99: traing loss: 1.7035366894901927e-08\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 100\n",
    "for epoch in range(n_epoch):\n",
    "    adam.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = loss_function(y_pred, y)\n",
    "    print(f\"Epoch {epoch}: traing loss: {loss}\")\n",
    "    loss.backward()\n",
    "    adam.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 1.4480,  0.8429,  1.4542,  1.3326,  0.9692],\n",
       "         [-0.7744, -0.2640, -0.6238, -0.7726, -0.6092],\n",
       "         [ 1.5298,  0.8408, -0.4859,  0.8626,  0.8568]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 1.4120, -0.7140,  1.8116], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([[1.4236, 0.2765, 1.8516],\n",
       "         [1.5264, 1.1963, 1.1501],\n",
       "         [1.6054, 0.3067, 1.1381],\n",
       "         [1.4456, 0.6615, 1.5413],\n",
       "         [1.5881, 0.3240, 1.9555]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([1.1975, 1.7264, 1.5122, 1.1803, 0.9093], requires_grad=True)]"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [0.9999, 0.9994, 0.9994, 0.9997, 0.9999],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model(x)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [0.9989, 0.9970, 0.9964, 0.9977, 0.9990],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 1.0000]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = y + torch.randn_like(y)\n",
    "y_pred = model(x2)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our raw data, which consists of sentences\n",
    "corpus = [\n",
    "    \"We always come to Paris\",\n",
    "    \"The professor is from Australia\",\n",
    "    \"I live in Stanford\",\n",
    "    \"He comes from Taiwan\",\n",
    "    \"The capital of Turkey is Ankara\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['we', 'always', 'come', 'to', 'paris'],\n",
       " ['the', 'professor', 'is', 'from', 'australia'],\n",
       " ['i', 'live', 'in', 'stanford'],\n",
       " ['he', 'comes', 'from', 'taiwan'],\n",
       " ['the', 'capital', 'of', 'turkey', 'is', 'ankara']]"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    return sentence.lower().split()\n",
    "\n",
    "\n",
    "train_sentences = [preprocess_sentence(sent) for sent in corpus]\n",
    "train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1, 0, 1]]"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set of locations that appear in our corpus\n",
    "locations = set([\"australia\", \"ankara\", \"paris\", \"stanford\", \"taiwan\", \"turkey\"])\n",
    "\n",
    "# Our train labels\n",
    "train_labels = [\n",
    "    [1 if word in locations else 0 for word in sent] for sent in train_sentences\n",
    "]\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'always',\n",
       " 'ankara',\n",
       " 'australia',\n",
       " 'capital',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'from',\n",
       " 'he',\n",
       " 'i',\n",
       " 'in',\n",
       " 'is',\n",
       " 'live',\n",
       " 'of',\n",
       " 'paris',\n",
       " 'professor',\n",
       " 'stanford',\n",
       " 'taiwan',\n",
       " 'the',\n",
       " 'to',\n",
       " 'turkey',\n",
       " 'we'}"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voca = set(w for s in train_sentences for w in s)\n",
    "voca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "voca.add(\"<unk>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<pad>', 'we', 'always', 'come', 'to', 'paris', '<pad>', '<pad>']"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voca.add(\"<pad>\")\n",
    "\n",
    "\n",
    "def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n",
    "    window = [pad_token] * window_size\n",
    "    return window + sentence + window\n",
    "\n",
    "\n",
    "window_size = 2\n",
    "pad_window(train_sentences[0], window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<unk>': 1,\n",
       " 'always': 2,\n",
       " 'ankara': 3,\n",
       " 'australia': 4,\n",
       " 'capital': 5,\n",
       " 'come': 6,\n",
       " 'comes': 7,\n",
       " 'from': 8,\n",
       " 'he': 9,\n",
       " 'i': 10,\n",
       " 'in': 11,\n",
       " 'is': 12,\n",
       " 'live': 13,\n",
       " 'of': 14,\n",
       " 'paris': 15,\n",
       " 'professor': 16,\n",
       " 'stanford': 17,\n",
       " 'taiwan': 18,\n",
       " 'the': 19,\n",
       " 'to': 20,\n",
       " 'turkey': 21,\n",
       " 'we': 22}"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_to_word = sorted(list(voca))\n",
    "\n",
    "# Creating a dictionary to find the index of a given word\n",
    "word_to_ix = {word: ind for ind, word in enumerate(ix_to_word)}\n",
    "word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence is: ['we', 'always', 'come', 'to', 'kuwait']\n",
      "Going from words to indices: [22, 2, 6, 20, 1]\n",
      "Going from indices to words: ['we', 'always', 'come', 'to', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "# Given a sentence of tokens, return the corresponding indices\n",
    "def convert_token_to_indices(sentence, word_to_ix):\n",
    "    indices = []\n",
    "    for token in sentence:\n",
    "        # Check if the token is in our vocabularly. If it is, get it's index.\n",
    "        # If not, get the index for the unknown token.\n",
    "        if token in word_to_ix:\n",
    "            index = word_to_ix[token]\n",
    "        else:\n",
    "            index = word_to_ix[\"<unk>\"]\n",
    "        indices.append(index)\n",
    "    return indices\n",
    "\n",
    "\n",
    "# More compact version of the same function\n",
    "def _convert_token_to_indices(sentence, word_to_ix):\n",
    "    return [word_to_ind.get(token, word_to_ix[\"<unk>\"]) for token in sentence]\n",
    "\n",
    "\n",
    "# Show an example\n",
    "example_sentence = [\"we\", \"always\", \"come\", \"to\", \"kuwait\"]\n",
    "example_indices = convert_token_to_indices(example_sentence, word_to_ix)\n",
    "restored_example = [ix_to_word[ind] for ind in example_indices]\n",
    "\n",
    "print(f\"Original sentence is: {example_sentence}\")\n",
    "print(f\"Going from words to indices: {example_indices}\")\n",
    "print(f\"Going from indices to words: {restored_example}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[22, 2, 6, 20, 15],\n",
       " [19, 16, 12, 8, 4],\n",
       " [10, 13, 11, 17],\n",
       " [9, 7, 8, 18],\n",
       " [19, 5, 14, 21, 12, 3]]"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting our sentences to indices\n",
    "example_padded_indices = [\n",
    "    convert_token_to_indices(s, word_to_ix) for s in train_sentences\n",
    "]\n",
    "example_padded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 9.8413e-01,  1.5484e+00, -1.2672e+00, -1.1082e+00,  8.3360e-01],\n",
       "         [ 6.9379e-01,  2.0860e+00,  9.0332e-01, -2.3859e-01,  2.1915e-01],\n",
       "         [ 1.8352e-01, -1.1562e-01, -2.1893e-01,  8.8643e-02,  9.9763e-01],\n",
       "         [-1.1169e-01, -4.4002e-01, -5.7400e-01, -3.1012e-01, -1.2741e+00],\n",
       "         [-2.9010e-01, -8.3380e-01, -1.3564e+00,  8.2453e-01, -5.9400e-01],\n",
       "         [-1.1549e+00,  7.9573e-01, -7.9232e-01,  8.3687e-04,  6.4324e-01],\n",
       "         [ 6.8193e-01,  1.5795e+00,  5.9015e-01, -9.4179e-01,  6.8130e-01],\n",
       "         [-1.2376e+00,  1.6334e+00,  9.5616e-01, -5.5822e-01, -9.8740e-02],\n",
       "         [-7.2223e-01,  1.2875e+00, -1.1724e+00, -2.0820e+00, -8.2918e-02],\n",
       "         [ 5.3661e-01, -1.1516e+00,  6.9675e-01, -2.2440e+00,  1.4616e+00],\n",
       "         [-2.7778e-01,  1.4968e+00, -5.9826e-01, -8.7479e-02, -6.6500e-02],\n",
       "         [-9.6939e-01,  9.7363e-01,  5.6244e-01, -8.8421e-01, -3.5952e-02],\n",
       "         [-1.2687e+00,  4.9372e-01,  2.2126e+00, -5.9502e-01, -2.5442e-01],\n",
       "         [-3.4805e-01, -1.3572e+00, -1.9953e-01, -1.8354e-01, -4.9494e-01],\n",
       "         [-1.0718e+00, -5.1265e-01,  2.4373e+00, -1.3560e+00,  2.2246e-02],\n",
       "         [ 1.1325e+00,  1.7271e+00, -1.4420e+00,  1.4533e-01, -1.1364e+00],\n",
       "         [-1.0863e+00, -4.0969e-01, -1.2425e+00, -1.0127e+00,  7.6362e-02],\n",
       "         [-1.9643e+00, -1.5067e+00, -8.0558e-01, -5.1634e-01,  7.9394e-01],\n",
       "         [-1.5870e+00,  1.1384e-01, -4.8078e-01, -1.4125e+00,  6.4148e-01],\n",
       "         [ 6.4107e-01,  7.0074e-01, -3.4801e-01, -2.3533e+00,  1.6861e+00],\n",
       "         [-4.6481e-01, -2.1076e+00,  5.6073e-01,  2.2999e+00,  4.1586e-01],\n",
       "         [-3.2678e-01,  1.6460e+00,  7.8731e-01,  1.1407e+00, -7.9835e-01],\n",
       "         [-3.4597e-01, -1.1105e+00, -6.7579e-01, -2.8699e-01,  1.7325e-01]],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 5\n",
    "embeds = nn.Embedding(len(voca), embedding_dim)\n",
    "\n",
    "list(embeds.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.1325,  1.7271, -1.4420,  0.1453, -1.1364],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = word_to_ix[\"paris\"]\n",
    "index_tensor = torch.tensor(index, dtype=torch.long)\n",
    "print(index_tensor.data)\n",
    "paris_embed = embeds(index_tensor)\n",
    "paris_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _custom_collate_fn(batch, window_size, word_to_ix):\n",
    "    # Prepare the datapoints\n",
    "    x, y = zip(*batch)\n",
    "    x = [pad_window(s, window_size=window_size) for s in x]\n",
    "    x = [convert_token_to_indices(s, word_to_ix) for s in x]\n",
    "\n",
    "    # Pad x so that all the examples in the batch have the same size\n",
    "    pad_token_ix = word_to_ix[\"<pad>\"]\n",
    "    x = [torch.LongTensor(x_i).cuda() for x_i in x]\n",
    "    x_padded = nn.utils.rnn.pad_sequence(\n",
    "        x, batch_first=True, padding_value=pad_token_ix\n",
    "    )\n",
    "\n",
    "    # Pad y and record the length\n",
    "    lengths = [len(label) for label in y]\n",
    "    lenghts = torch.LongTensor(lengths)\n",
    "    y = [torch.LongTensor(y_i).cuda() for y_i in y]\n",
    "    y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
    "\n",
    "    return x_padded, y_padded, lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Batched Input:\n",
      "tensor([[ 0,  0,  9,  7,  8, 18,  0,  0],\n",
      "        [ 0,  0, 10, 13, 11, 17,  0,  0]], device='cuda:0')\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 1],\n",
      "        [0, 0, 0, 1]], device='cuda:0')\n",
      "Batched Lengths:\n",
      "tensor([4, 4])\n",
      "\n",
      "Iteration 1\n",
      "Batched Input:\n",
      "tensor([[ 0,  0, 19, 16, 12,  8,  4,  0,  0,  0],\n",
      "        [ 0,  0, 19,  5, 14, 21, 12,  3,  0,  0]], device='cuda:0')\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 1, 0, 1]], device='cuda:0')\n",
      "Batched Lengths:\n",
      "tensor([5, 6])\n",
      "\n",
      "Iteration 2\n",
      "Batched Input:\n",
      "tensor([[ 0,  0, 22,  2,  6, 20, 15,  0,  0]], device='cuda:0')\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 0, 1]], device='cuda:0')\n",
      "Batched Lengths:\n",
      "tensor([5])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "data = list(zip(train_sentences, train_labels))\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "window_size = 2\n",
    "collate_fn = partial(_custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n",
    "\n",
    "# Instantiate the DataLoader\n",
    "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "# Go through one loop\n",
    "counter = 0\n",
    "for batched_x, batched_y, batched_lengths in loader:\n",
    "    print(f\"Iteration {counter}\")\n",
    "    print(\"Batched Input:\")\n",
    "    print(batched_x)\n",
    "    print(\"Batched Labels:\")\n",
    "    print(batched_y)\n",
    "    print(\"Batched Lengths:\")\n",
    "    print(batched_lengths)\n",
    "    print(\"\")\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor: \n",
      "tensor([[ 0,  0, 22,  2,  6, 20, 15,  0,  0]], device='cuda:0')\n",
      "\n",
      "Windows: \n",
      "tensor([[[ 0,  0, 22,  2,  6],\n",
      "         [ 0, 22,  2,  6, 20],\n",
      "         [22,  2,  6, 20, 15],\n",
      "         [ 2,  6, 20, 15,  0],\n",
      "         [ 6, 20, 15,  0,  0]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Print the original tensor\n",
    "print(f\"Original Tensor: \")\n",
    "print(batched_x)\n",
    "print(\"\")\n",
    "\n",
    "# Create the 2 * 2 + 1 chunks\n",
    "chunk = batched_x.unfold(1, window_size * 2 + 1, 1)\n",
    "print(f\"Windows: \")\n",
    "print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3090'"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "\n",
    "class WordWindowClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, hyperparameters, vocab_size, pad_ix=0):\n",
    "        super(WordWindowClassifier, self).__init__()\n",
    "\n",
    "        \"\"\" Instance variables \"\"\"\n",
    "        self.window_size = hyperparameters[\"window_size\"]\n",
    "        self.embed_dim = hyperparameters[\"embed_dim\"]\n",
    "        self.hidden_dim = hyperparameters[\"hidden_dim\"]\n",
    "        self.freeze_embeddings = hyperparameters[\"freeze_embeddings\"]\n",
    "\n",
    "        \"\"\" Embedding Layer\n",
    "    Takes in a tensor containing embedding indices, and returns the\n",
    "    corresponding embeddings. The output is of dim\n",
    "    (number_of_indices * embedding_dim).\n",
    "\n",
    "    If freeze_embeddings is True, set the embedding layer parameters to be\n",
    "    non-trainable. This is useful if we only want the parameters other than the\n",
    "    embeddings parameters to change.\n",
    "\n",
    "    \"\"\"\n",
    "        self.embeds = nn.Embedding(vocab_size, self.embed_dim, padding_idx=pad_ix)\n",
    "        if self.freeze_embeddings:\n",
    "            self.embed_layer.weight.requires_grad = False\n",
    "\n",
    "        \"\"\" Hidden Layer\n",
    "    \"\"\"\n",
    "        full_window_size = 2 * window_size + 1\n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Linear(full_window_size * self.embed_dim, self.hidden_dim), nn.Tanh()\n",
    "        )\n",
    "\n",
    "        \"\"\" Output Layer\n",
    "    \"\"\"\n",
    "        self.output_layer = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "        \"\"\" Probabilities\n",
    "    \"\"\"\n",
    "        self.probabilities = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Let B:= batch_size\n",
    "            L:= window-padded sentence length\n",
    "            D:= self.embed_dim\n",
    "            S:= self.window_size\n",
    "            H:= self.hidden_dim\n",
    "\n",
    "        inputs: a (B, L) tensor of token indices\n",
    "        \"\"\"\n",
    "        B, L = inputs.size()\n",
    "\n",
    "        \"\"\"\n",
    "    Reshaping.\n",
    "    Takes in a (B, L) LongTensor\n",
    "    Outputs a (B, L~, S) LongTensor\n",
    "    \"\"\"\n",
    "        # Fist, get our word windows for each word in our input.\n",
    "        token_windows = inputs.unfold(1, 2 * self.window_size + 1, 1)\n",
    "        _, adjusted_length, _ = token_windows.size()\n",
    "\n",
    "        # Good idea to do internal tensor-size sanity checks, at the least in comments!\n",
    "        # assert token_windows.size() == (B, adjusted_length, 2 * self.window_size + 1)\n",
    "\n",
    "        \"\"\"\n",
    "    Embedding.\n",
    "    Takes in a torch.LongTensor of size (B, L~, S)\n",
    "    Outputs a (B, L~, S, D) FloatTensor.\n",
    "    \"\"\"\n",
    "        embedded_windows = self.embeds(token_windows)\n",
    "\n",
    "        \"\"\"\n",
    "    Reshaping.\n",
    "    Takes in a (B, L~, S, D) FloatTensor.\n",
    "    Resizes it into a (B, L~, S*D) FloatTensor.\n",
    "    -1 argument \"infers\" what the last dimension should be based on leftover axes.\n",
    "    \"\"\"\n",
    "        embedded_windows = embedded_windows.view(B, adjusted_length, -1)\n",
    "\n",
    "        \"\"\"\n",
    "    Layer 1.\n",
    "    Takes in a (B, L~, S*D) FloatTensor.\n",
    "    Resizes it into a (B, L~, H) FloatTensor\n",
    "    \"\"\"\n",
    "        layer_1 = self.hidden_layer(embedded_windows)\n",
    "\n",
    "        \"\"\"\n",
    "    Layer 2\n",
    "    Takes in a (B, L~, H) FloatTensor.\n",
    "    Resizes it into a (B, L~, 1) FloatTensor.\n",
    "    \"\"\"\n",
    "        output = self.output_layer(layer_1)\n",
    "\n",
    "        \"\"\"\n",
    "    Softmax.\n",
    "    Takes in a (B, L~, 1) FloatTensor of unnormalized class scores.\n",
    "    Outputs a (B, L~, 1) FloatTensor of (log-)normalized class scores.\n",
    "    \"\"\"\n",
    "        output = self.probabilities(output)\n",
    "        output = output.view(B, -1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "data = list(zip(train_sentences, train_labels))\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "window_size = 2\n",
    "collate_fn = partial(_custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n",
    "\n",
    "# Instantiate a DataLoader\n",
    "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize a model\n",
    "# It is useful to put all the model hyperparameters in a dictionary\n",
    "model_hyperparameters = {\n",
    "    \"batch_size\": 4,\n",
    "    \"window_size\": 2,\n",
    "    \"embed_dim\": 25,\n",
    "    \"hidden_dim\": 25,\n",
    "    \"freeze_embeddings\": False,\n",
    "}\n",
    "\n",
    "vocab_size = len(word_to_ix)\n",
    "\n",
    "model = WordWindowClassifier(model_hyperparameters, vocab_size)\n",
    "model.to(device)\n",
    "\n",
    "# Define an optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "# Define a loss function, which computes to binary cross entropy loss\n",
    "def loss_function(batch_outputs, batch_labels, batch_lengths):\n",
    "    # Calculate the loss for the whole batch\n",
    "    bceloss = nn.BCELoss()\n",
    "    loss = bceloss(batch_outputs, batch_labels.float())\n",
    "\n",
    "    # Rescale the loss. Remember that we have used lengths to store the\n",
    "    # number of words in each training example\n",
    "    loss = loss / batch_lengths.sum().float()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that will be called in every epoch\n",
    "def train_epoch(loss_function, optimizer, model, loader):\n",
    "\n",
    "    # Keep track of the total loss for the batch\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_labels, batch_lengths in loader:\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Run a forward pass\n",
    "        outputs = model.forward(batch_inputs)\n",
    "        # Compute the batch loss\n",
    "        loss = loss_function(outputs, batch_labels, batch_lengths)\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "        # Update the parameteres\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# Function containing our main training loop\n",
    "def train(loss_function, optimizer, model, loader, num_epochs=10000):\n",
    "\n",
    "    # Iterate through each epoch and call our train_epoch function\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = train_epoch(loss_function, optimizer, model, loader)\n",
    "        if epoch % 100 == 0:\n",
    "            print(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[421], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m----> 2\u001b[0m train(loss_function, optimizer, model, loader, num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs)\n",
      "Cell \u001b[0;32mIn[420], line 27\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(loss_function, optimizer, model, loader, num_epochs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(loss_function, optimizer, model, loader, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m):\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Iterate through each epoch and call our train_epoch function\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 27\u001b[0m         epoch_loss \u001b[38;5;241m=\u001b[39m train_epoch(loss_function, optimizer, model, loader)\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;28mprint\u001b[39m(epoch_loss)\n",
      "Cell \u001b[0;32mIn[420], line 10\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(loss_function, optimizer, model, loader)\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Run a forward pass\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(batch_inputs)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Compute the batch loss\u001b[39;00m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, batch_labels, batch_lengths)\n",
      "Cell \u001b[0;32mIn[418], line 73\u001b[0m, in \u001b[0;36mWordWindowClassifier.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Good idea to do internal tensor-size sanity checks, at the least in comments!\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# assert token_windows.size() == (B, adjusted_length, 2 * self.window_size + 1)\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03mEmbedding.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03mTakes in a torch.LongTensor of size (B, L~, S)\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03mOutputs a (B, L~, S, D) FloatTensor.\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     embedded_windows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeds(token_windows)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03mReshaping.\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03mTakes in a (B, L~, S, D) FloatTensor.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03mResizes it into a (B, L~, S*D) FloatTensor.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m-1 argument \"infers\" what the last dimension should be based on leftover axes.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     embedded_windows \u001b[38;5;241m=\u001b[39m embedded_windows\u001b[38;5;241m.\u001b[39mview(B, adjusted_length, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/cs224n/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/cs224n/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/cs224n/lib/python3.12/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse)\n",
      "File \u001b[0;32m/opt/conda/envs/cs224n/lib/python3.12/site-packages/torch/nn/functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "train(loss_function, optimizer, model, loader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test sentences\n",
    "test_corpus = [\"She comes from Paris\"]\n",
    "test_sentences = [s.lower().split() for s in test_corpus]\n",
    "test_labels = [[0, 0, 0, 1]]\n",
    "\n",
    "# Create a test loader\n",
    "test_data = list(zip(test_sentences, test_labels))\n",
    "batch_size = 1\n",
    "shuffle = False\n",
    "window_size = 2\n",
    "collate_fn = partial(_custom_collate_fn, window_size=2, word_to_ix=word_to_ix)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=1, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 1]])\n",
      "tensor([[0.1176, 0.0254, 0.0952, 0.8463]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for test_instance, labels, _ in test_loader:\n",
    "    outputs = model.forward(test_instance)\n",
    "    print(labels)\n",
    "    print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
